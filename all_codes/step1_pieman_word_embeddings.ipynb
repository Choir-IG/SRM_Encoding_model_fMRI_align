{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42b42b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1057 > 1024). Running this sequence through the model will result in indexing errors\n",
      "C:\\Users\\CBS\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\CBS\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d321d5d0af6748c7b5f4da06b6e0dac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  21%|##1       | 1.35G/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CBS\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\CBS\\.cache\\huggingface\\hub\\models--gpt2-xl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Extracting token embeddings (sliding window): 100%|██████████| 5/5 [00:22<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP1 DONE]\n",
      "Words: 957 | Tokens: 1057 | Hidden size: 1600\n",
      "Missing timing rows: 3\n",
      "Words with no tokens mapped: 0\n",
      "Saved CSV: E:\\Nastase\\encoding_features\\pieman_step1\\pieman_word_table.csv\n",
      "Saved token embeddings: pieman_token_emb_lastlayer.npy\n",
      "Saved word embeddings:  pieman_word_emb_lastlayer.npy\n",
      "Saved figures: fig_word_duration_hist.png, fig_word_token_count_hist.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Step 1: Build word-level table from Gentle align file and extract GPT2-XL embeddings.\n",
    "- Reads align.csv or align.xlsx (Gentle word-level output)\n",
    "- Builds a clean word table with onset/offset timing\n",
    "- Extracts token-level contextual embeddings from GPT2-XL using sliding window (max context 1024)\n",
    "- Pools token embeddings into word embeddings by mean pooling\n",
    "- Saves CSV + NPY outputs + sanity-check plots\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# -----------------------------\n",
    "# User config (EDIT THESE)\n",
    "# -----------------------------\n",
    "ALIGN_PATH = r\"E:\\Nastase\\align\\gentle\\pieman\\align.csv\"   # <- change to your file (align.csv or align.xlsx)\n",
    "OUT_DIR = r\"E:\\Nastase\\encoding_features\\pieman_step1\"     # <- output directory\n",
    "MODEL_NAME = \"gpt2-xl\"                                     # you said GPT2-XL is installed\n",
    "\n",
    "# Sliding window settings for GPT-2 (max positions 1024)\n",
    "MAX_CTX_TOKENS = 1024\n",
    "STRIDE = 256  # tokens per step to compute representations; smaller = slower but safer\n",
    "BATCH_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Plot settings (keep consistent binning/range across multiple figures)\n",
    "DURATION_BINS = 50\n",
    "TOKENCOUNT_BINS = 30\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def read_gentle_align(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read Gentle word-level alignment table.\n",
    "    Gentle CSV has 4 columns with no header:\n",
    "      0 = transcript word\n",
    "      1 = alignedWord (or <unk>)\n",
    "      2 = start (sec)\n",
    "      3 = end (sec)\n",
    "    Some rows may have missing start/end if not found in audio.\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".xlsx\", \".xls\"]:\n",
    "        df = pd.read_excel(path, header=None)\n",
    "    else:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"Align file has too few columns: {df.shape[1]}\")\n",
    "\n",
    "    # Ensure exactly 4 columns (pad if needed)\n",
    "    while df.shape[1] < 4:\n",
    "        df[df.shape[1]] = np.nan\n",
    "    df = df.iloc[:, :4].copy()\n",
    "    df.columns = [\"transcript_word\", \"aligned_word\", \"start_sec\", \"end_sec\"]\n",
    "\n",
    "    # Normalize types\n",
    "    df[\"transcript_word\"] = df[\"transcript_word\"].astype(str)\n",
    "    df[\"aligned_word\"] = df[\"aligned_word\"].astype(str)\n",
    "\n",
    "    # Coerce timing to numeric (NaNs allowed)\n",
    "    df[\"start_sec\"] = pd.to_numeric(df[\"start_sec\"], errors=\"coerce\")\n",
    "    df[\"end_sec\"] = pd.to_numeric(df[\"end_sec\"], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_transcript_text(words: List[str]) -> Tuple[str, List[Tuple[int, int]]]:\n",
    "    \"\"\"\n",
    "    Build a single transcript string by joining words with single spaces,\n",
    "    and return character spans (start_char, end_char) for each word in that string.\n",
    "\n",
    "    This allows mapping token offsets -> words using tokenizer offset mapping.\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    parts = []\n",
    "    cursor = 0\n",
    "    for i, w in enumerate(words):\n",
    "        if i > 0:\n",
    "            parts.append(\" \")\n",
    "            cursor += 1\n",
    "        start = cursor\n",
    "        parts.append(w)\n",
    "        cursor += len(w)\n",
    "        end = cursor\n",
    "        spans.append((start, end))\n",
    "    text = \"\".join(parts)\n",
    "    return text, spans\n",
    "\n",
    "\n",
    "def map_tokens_to_words(offsets: List[Tuple[int, int]], word_spans: List[Tuple[int, int]]) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Map each word to the list of token indices whose character offsets overlap the word span.\n",
    "    Token offsets come from tokenizer(..., return_offsets_mapping=True).\n",
    "    \"\"\"\n",
    "    word_to_tokens: List[List[int]] = [[] for _ in word_spans]\n",
    "\n",
    "    # Two-pointer sweep for efficiency\n",
    "    w = 0\n",
    "    for t_idx, (t0, t1) in enumerate(offsets):\n",
    "        if t1 <= t0:\n",
    "            # Skip special/empty tokens (GPT2 usually none, but keep safe)\n",
    "            continue\n",
    "\n",
    "        # Advance word pointer if token is beyond current word\n",
    "        while w < len(word_spans) and word_spans[w][1] <= t0:\n",
    "            w += 1\n",
    "        if w >= len(word_spans):\n",
    "            break\n",
    "\n",
    "        # Token may overlap multiple spans in rare cases; handle via local scan\n",
    "        ww = w\n",
    "        while ww < len(word_spans):\n",
    "            w0, w1 = word_spans[ww]\n",
    "            # Stop if word starts after token ends\n",
    "            if w0 >= t1:\n",
    "                break\n",
    "            # Overlap condition\n",
    "            if (t0 < w1) and (t1 > w0):\n",
    "                word_to_tokens[ww].append(t_idx)\n",
    "            ww += 1\n",
    "\n",
    "    return word_to_tokens\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_token_embeddings_sliding_window(\n",
    "    model: AutoModel,\n",
    "    input_ids: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    max_ctx: int = 1024,\n",
    "    stride: int = 256,\n",
    "    device: str = \"cpu\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract token-level contextual embeddings using a sliding window.\n",
    "    Returns:\n",
    "      token_emb: (T, H) float32, embeddings from the last hidden layer.\n",
    "\n",
    "    Strategy:\n",
    "      For token positions in a target block, run model on a window that contains up to max_ctx tokens,\n",
    "      then take embeddings for the target positions.\n",
    "    \"\"\"\n",
    "    T = input_ids.shape[0]\n",
    "    H = model.config.hidden_size\n",
    "    token_emb = np.zeros((T, H), dtype=np.float32)\n",
    "\n",
    "    # Ensure tensors on device\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    # Iterate over target ranges\n",
    "    # We compute representations for tokens in [i, i+stride)\n",
    "    for i in tqdm(range(0, T, stride), desc=\"Extracting token embeddings (sliding window)\"):\n",
    "        tgt_start = i\n",
    "        tgt_end = min(i + stride, T)\n",
    "\n",
    "        # Window end aligns with tgt_end to maximize left context for the target tokens\n",
    "        win_end = tgt_end\n",
    "        win_start = max(0, win_end - max_ctx)\n",
    "\n",
    "        ids_win = input_ids[win_start:win_end].unsqueeze(0)          # (1, L)\n",
    "        mask_win = attention_mask[win_start:win_end].unsqueeze(0)    # (1, L)\n",
    "\n",
    "        out = model(input_ids=ids_win, attention_mask=mask_win, output_hidden_states=False)\n",
    "        # For GPT2Model, out.last_hidden_state is (1, L, H)\n",
    "        last = out.last_hidden_state[0]  # (L, H)\n",
    "\n",
    "        # Indices in the window that correspond to target tokens\n",
    "        rel_start = tgt_start - win_start\n",
    "        rel_end = tgt_end - win_start\n",
    "\n",
    "        token_emb[tgt_start:tgt_end, :] = last[rel_start:rel_end, :].detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    return token_emb\n",
    "\n",
    "\n",
    "def plot_histograms(word_df: pd.DataFrame, out_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Create two sanity-check histograms:\n",
    "    1) word duration distribution (seconds)\n",
    "    2) token count per word distribution\n",
    "    Use fixed bin counts for consistent ranges.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Duration histogram (ignore NaNs)\n",
    "    durations = word_df[\"duration_sec\"].dropna().values\n",
    "    if durations.size > 0:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(durations, bins=DURATION_BINS)\n",
    "        plt.title(\"Word duration distribution (sec)\")\n",
    "        plt.xlabel(\"Duration (sec)\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, \"fig_word_duration_hist.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    # Token count histogram\n",
    "    tok_counts = word_df[\"n_tokens\"].fillna(0).astype(int).values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(tok_counts, bins=TOKENCOUNT_BINS)\n",
    "    plt.title(\"Token count per word (GPT2 BPE)\")\n",
    "    plt.xlabel(\"Number of tokens\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"fig_word_token_count_hist.png\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) Read alignment table\n",
    "    # -----------------------------\n",
    "    align_df = read_gentle_align(ALIGN_PATH)\n",
    "\n",
    "    # Basic word table\n",
    "    word_df = align_df.copy()\n",
    "    word_df.insert(0, \"word_id\", np.arange(len(word_df), dtype=int))\n",
    "\n",
    "    # Flags\n",
    "    word_df[\"is_unk\"] = word_df[\"aligned_word\"].str.strip().eq(\"<unk>\")\n",
    "    word_df[\"has_timing\"] = word_df[\"start_sec\"].notna() & word_df[\"end_sec\"].notna()\n",
    "    word_df[\"duration_sec\"] = word_df[\"end_sec\"] - word_df[\"start_sec\"]\n",
    "\n",
    "    # Use transcript_word as the canonical word string (keeps punctuation etc.)\n",
    "    words = word_df[\"transcript_word\"].tolist()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) Build transcript text + word spans\n",
    "    # -----------------------------\n",
    "    transcript_text, word_spans = build_transcript_text(words)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) Tokenize with offset mapping (fast tokenizer)\n",
    "    # -----------------------------\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "    # GPT-2 has no pad token by default; we do not need padding for a single sequence\n",
    "    enc = tokenizer(\n",
    "        transcript_text,\n",
    "        add_special_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"][0]            # (T,)\n",
    "    attention_mask = enc[\"attention_mask\"][0]  # (T,)\n",
    "    offsets = enc[\"offset_mapping\"][0].tolist()  # list of (start_char, end_char)\n",
    "\n",
    "    T = int(input_ids.shape[0])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) Map tokens -> words by character overlap\n",
    "    # -----------------------------\n",
    "    word_to_tokens = map_tokens_to_words(offsets, word_spans)\n",
    "\n",
    "    # Fill token counts\n",
    "    word_df[\"n_tokens\"] = [len(toks) for toks in word_to_tokens]\n",
    "    word_df[\"has_tokens\"] = word_df[\"n_tokens\"] > 0\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) Load GPT2-XL model and extract token embeddings (last layer) with sliding window\n",
    "    # -----------------------------\n",
    "    device = BATCH_DEVICE\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    token_emb = extract_token_embeddings_sliding_window(\n",
    "        model=model,\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_ctx=MAX_CTX_TOKENS,\n",
    "        stride=STRIDE,\n",
    "        device=device,\n",
    "    )  # (T, H)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6) Token -> word mean pooling (last layer)\n",
    "    # -----------------------------\n",
    "    H = token_emb.shape[1]\n",
    "    W = len(word_to_tokens)\n",
    "    word_emb = np.full((W, H), np.nan, dtype=np.float32)\n",
    "\n",
    "    for w_idx, toks in enumerate(word_to_tokens):\n",
    "        if len(toks) == 0:\n",
    "            continue\n",
    "        word_emb[w_idx, :] = token_emb[toks, :].mean(axis=0).astype(np.float32)\n",
    "\n",
    "    # Add simple embedding norm summary for quick diagnostics\n",
    "    word_df[\"emb_l2norm_lastlayer\"] = np.linalg.norm(np.nan_to_num(word_emb, nan=0.0), axis=1)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7) Save outputs\n",
    "    # -----------------------------\n",
    "    out_csv = os.path.join(OUT_DIR, \"pieman_word_table.csv\")\n",
    "    word_df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    np.save(os.path.join(OUT_DIR, \"pieman_token_emb_lastlayer.npy\"), token_emb)\n",
    "    np.save(os.path.join(OUT_DIR, \"pieman_word_emb_lastlayer.npy\"), word_emb)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 8) Plot sanity-check figures\n",
    "    # -----------------------------\n",
    "    plot_histograms(word_df, OUT_DIR)\n",
    "\n",
    "    # Print a compact summary\n",
    "    n_missing_timing = int((~word_df[\"has_timing\"]).sum())\n",
    "    n_missing_tokens = int((~word_df[\"has_tokens\"]).sum())\n",
    "    print(\"\\n[STEP1 DONE]\")\n",
    "    print(f\"Words: {len(word_df)} | Tokens: {T} | Hidden size: {H}\")\n",
    "    print(f\"Missing timing rows: {n_missing_timing}\")\n",
    "    print(f\"Words with no tokens mapped: {n_missing_tokens}\")\n",
    "    print(f\"Saved CSV: {out_csv}\")\n",
    "    print(f\"Saved token embeddings: pieman_token_emb_lastlayer.npy\")\n",
    "    print(f\"Saved word embeddings:  pieman_word_emb_lastlayer.npy\")\n",
    "    print(\"Saved figures: fig_word_duration_hist.png, fig_word_token_count_hist.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac9a98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting protobuf==3.20.3\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.33.5\n",
      "    Uninstalling protobuf-6.33.5:\n",
      "      Successfully uninstalled protobuf-6.33.5\n",
      "Successfully installed protobuf-3.20.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\CBS\\AppData\\Roaming\\Python\\Python311\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-proto 1.39.1 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n",
      "streamlit 1.30.0 requires rich<14,>=10.14.0, but you have rich 14.3.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"protobuf==3.20.3\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
