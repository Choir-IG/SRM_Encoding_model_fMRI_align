{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5027a497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP3 DONE]\n",
      "TR: 1.5s | N_TR: 300 | Total: 450.0s | Time origin shift: 0.0s\n",
      "Words: 957 | Layers: 49 | PCA dim: 50\n",
      "Non-empty TR bins: 259/300\n",
      "Saved TR features: pieman_tr_features_pca50.npy\n",
      "Saved TR summary:  E:\\Nastase\\encoding_features\\pieman_step3\\pieman_tr_summary.csv\n",
      "Saved TRxLayer CSV (gz): E:\\Nastase\\encoding_features\\pieman_step3\\pieman_tr_layer_features_pca50.csv.gz\n",
      "Saved figures: fig_words_per_tr.png, fig_tr_feature_norm_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Step 3: Aggregate word-level PCA(50) features into TR-level features (Pie Man).\n",
    "- Loads Step 1 word table for word timing (start_sec/end_sec)\n",
    "- Loads Step 2 word PCA features: (W, L, 50)\n",
    "- Builds TR bins for Pie Man (default: TR=1.5s, N_TR=300, total=450s)\n",
    "- Duration-weighted aggregation by overlap between word time interval and TR bin\n",
    "- Saves:\n",
    "  1) X_tr.npy: (N_TR, L, 50)\n",
    "  2) tr_layer_features.csv.gz: long table (N_TR * L rows, 50 dims + metadata)\n",
    "  3) summary CSV (words per TR, coverage per TR)\n",
    "- Plots:\n",
    "  - Words per TR line plot\n",
    "  - Heatmap of TR feature L2 norms (TR x layer) with robust color range\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# User config (EDIT THESE)\n",
    "# -----------------------------\n",
    "STEP1_DIR = r\"E:\\Nastase\\encoding_features\\pieman_step1\"\n",
    "STEP2_DIR = r\"E:\\Nastase\\encoding_features\\pieman_step2\"\n",
    "OUT_DIR   = r\"E:\\Nastase\\encoding_features\\pieman_step3\"\n",
    "\n",
    "TR_SEC = 1.5\n",
    "N_TR = 300\n",
    "TOTAL_SEC = TR_SEC * N_TR  # 450s for Pie Man\n",
    "\n",
    "# Optional: shift time origin (seconds) if you want story onset to be time 0.\n",
    "# For Pie Man, story starts at 15s after intro music+silence.\n",
    "# Keep 0.0 for now (recommended) to preserve the actual audio timeline.\n",
    "TIME_ORIGIN_SEC = 0.0\n",
    "\n",
    "FIG_DPI = 220\n",
    "\n",
    "\n",
    "def compute_overlap(a0: float, a1: float, b0: float, b1: float) -> float:\n",
    "    \"\"\"Return overlap duration between intervals [a0,a1] and [b0,b1].\"\"\"\n",
    "    left = max(a0, b0)\n",
    "    right = min(a1, b1)\n",
    "    return max(0.0, right - left)\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) Load word table (timing)\n",
    "    # -----------------------------\n",
    "    word_csv = os.path.join(STEP1_DIR, \"pieman_word_table.csv\")\n",
    "    word_df = pd.read_csv(word_csv)\n",
    "\n",
    "    # Timing arrays (seconds)\n",
    "    start = pd.to_numeric(word_df[\"start_sec\"], errors=\"coerce\").to_numpy(dtype=float)\n",
    "    end = pd.to_numeric(word_df[\"end_sec\"], errors=\"coerce\").to_numpy(dtype=float)\n",
    "\n",
    "    # Apply time origin shift if requested\n",
    "    start = start - TIME_ORIGIN_SEC\n",
    "    end = end - TIME_ORIGIN_SEC\n",
    "\n",
    "    has_timing = np.isfinite(start) & np.isfinite(end) & (end > start)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) Load word PCA features (W, L, 50)\n",
    "    # -----------------------------\n",
    "    pca_path = os.path.join(STEP2_DIR, \"pieman_word_emb_all_layers_pca50.npy\")\n",
    "    word_pca = np.load(pca_path)  # shape (W, L, 50)\n",
    "    W, L, D = word_pca.shape\n",
    "    assert D == 50, f\"Expected PCA dim=50, got {D}\"\n",
    "    assert len(word_df) == W, f\"Word count mismatch: CSV={len(word_df)} vs PCA={W}\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) Build TR bins\n",
    "    # -----------------------------\n",
    "    tr_starts = np.arange(N_TR, dtype=float) * TR_SEC\n",
    "    tr_ends = tr_starts + TR_SEC\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) Duration-weighted TR aggregation\n",
    "    # -----------------------------\n",
    "    X_tr = np.zeros((N_TR, L, D), dtype=np.float32)\n",
    "    words_per_tr = np.zeros((N_TR,), dtype=np.int32)\n",
    "    coverage_sec_per_tr = np.zeros((N_TR,), dtype=np.float32)\n",
    "\n",
    "    # Also store per TR per layer total weight for diagnostics\n",
    "    weight_tr_layer = np.zeros((N_TR, L), dtype=np.float32)\n",
    "\n",
    "    # Iterate over TRs (Pie Man: 300 TRs, fast enough)\n",
    "    for t in range(N_TR):\n",
    "        b0, b1 = tr_starts[t], tr_ends[t]\n",
    "\n",
    "        # Find candidate words whose intervals could overlap this TR bin\n",
    "        # (Simple scan is fine at W=957; keep it explicit for clarity)\n",
    "        weights = np.zeros((W,), dtype=np.float32)\n",
    "\n",
    "        for i in range(W):\n",
    "            if not has_timing[i]:\n",
    "                continue\n",
    "            w = compute_overlap(start[i], end[i], b0, b1)\n",
    "            if w > 0:\n",
    "                weights[i] = w\n",
    "\n",
    "        idx = np.nonzero(weights > 0)[0]\n",
    "        if idx.size == 0:\n",
    "            continue\n",
    "\n",
    "        # Count how many unique words contribute to this TR\n",
    "        words_per_tr[t] = int(idx.size)\n",
    "        coverage_sec_per_tr[t] = float(weights[idx].sum())\n",
    "\n",
    "        # Normalize weights to compute a weighted mean\n",
    "        wsum = float(weights[idx].sum())\n",
    "        wn = (weights[idx] / wsum).astype(np.float32)  # (n_words,)\n",
    "\n",
    "        # Weighted mean aggregation for each layer\n",
    "        # word_pca[idx] -> (n_words, L, D)\n",
    "        # We compute sum_i wn_i * word_pca_i\n",
    "        # Use einsum for clarity and speed.\n",
    "        X_tr[t] = np.einsum(\"i,ild->ld\", wn, word_pca[idx]).astype(np.float32)\n",
    "\n",
    "        # Diagnostics: total unnormalized weight per layer equals coverage_sec (same for all layers)\n",
    "        # but we store it anyway for later extensions\n",
    "        weight_tr_layer[t, :] = coverage_sec_per_tr[t]\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) Save TR features\n",
    "    # -----------------------------\n",
    "    np.save(os.path.join(OUT_DIR, \"pieman_tr_features_pca50.npy\"), X_tr)\n",
    "\n",
    "    # Save a compact summary CSV per TR\n",
    "    tr_summary = pd.DataFrame({\n",
    "        \"tr_idx\": np.arange(N_TR, dtype=int),\n",
    "        \"tr_start_sec\": tr_starts,\n",
    "        \"tr_end_sec\": tr_ends,\n",
    "        \"n_words\": words_per_tr,\n",
    "        \"coverage_sec\": coverage_sec_per_tr,\n",
    "    })\n",
    "    tr_summary_path = os.path.join(OUT_DIR, \"pieman_tr_summary.csv\")\n",
    "    tr_summary.to_csv(tr_summary_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Save a long-form CSV: TR x layer rows, 50 dims columns (gzip compressed)\n",
    "    # Shape: 300*49 = 14700 rows, each has 50 values + metadata\n",
    "    rows = []\n",
    "    for t in range(N_TR):\n",
    "        for l in range(L):\n",
    "            row = {\n",
    "                \"tr_idx\": t,\n",
    "                \"tr_start_sec\": tr_starts[t],\n",
    "                \"tr_end_sec\": tr_ends[t],\n",
    "                \"layer\": l,\n",
    "                \"n_words\": int(words_per_tr[t]),\n",
    "                \"coverage_sec\": float(coverage_sec_per_tr[t]),\n",
    "            }\n",
    "            # Add PC columns\n",
    "            vec = X_tr[t, l, :]\n",
    "            for k in range(D):\n",
    "                row[f\"PC{k+1:02d}\"] = float(vec[k])\n",
    "            rows.append(row)\n",
    "\n",
    "    tr_layer_df = pd.DataFrame(rows)\n",
    "    tr_layer_csv_gz = os.path.join(OUT_DIR, \"pieman_tr_layer_features_pca50.csv.gz\")\n",
    "    tr_layer_df.to_csv(tr_layer_csv_gz, index=False, compression=\"gzip\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6) Plot 1: words per TR timecourse\n",
    "    # -----------------------------\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(np.arange(N_TR), words_per_tr)\n",
    "    plt.xlabel(\"TR index\")\n",
    "    plt.ylabel(\"Number of contributing words\")\n",
    "    plt.title(\"Pie Man: word coverage per TR (duration-weighted overlap)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"fig_words_per_tr.png\"), dpi=FIG_DPI)\n",
    "    plt.close()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7) Plot 2: heatmap of TR feature norms (TR x layer)\n",
    "    # -----------------------------\n",
    "    # Compute L2 norms across PCA dims: (N_TR, L)\n",
    "    norms = np.linalg.norm(X_tr, axis=2)\n",
    "\n",
    "    # Use robust color range for consistent interpretation\n",
    "    vmin = float(np.quantile(norms, 0.01))\n",
    "    vmax = float(np.quantile(norms, 0.99))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(norms.T, aspect=\"auto\", origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(label=\"L2 norm of TR feature (PCA50)\")\n",
    "    plt.xlabel(\"TR index\")\n",
    "    plt.ylabel(\"Layer (0=embeddings, 48=top)\")\n",
    "    plt.title(\"TR-level feature strength heatmap (robust scaled)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"fig_tr_feature_norm_heatmap.png\"), dpi=FIG_DPI)\n",
    "    plt.close()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Print a compact summary\n",
    "    # -----------------------------\n",
    "    nonempty_tr = int((words_per_tr > 0).sum())\n",
    "    print(\"\\n[STEP3 DONE]\")\n",
    "    print(f\"TR: {TR_SEC}s | N_TR: {N_TR} | Total: {TOTAL_SEC:.1f}s | Time origin shift: {TIME_ORIGIN_SEC}s\")\n",
    "    print(f\"Words: {W} | Layers: {L} | PCA dim: {D}\")\n",
    "    print(f\"Non-empty TR bins: {nonempty_tr}/{N_TR}\")\n",
    "    print(f\"Saved TR features: pieman_tr_features_pca50.npy\")\n",
    "    print(f\"Saved TR summary:  {tr_summary_path}\")\n",
    "    print(f\"Saved TRxLayer CSV (gz): {tr_layer_csv_gz}\")\n",
    "    print(\"Saved figures: fig_words_per_tr.png, fig_tr_feature_norm_heatmap.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
