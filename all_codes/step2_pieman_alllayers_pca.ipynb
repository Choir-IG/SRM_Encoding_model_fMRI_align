{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c86ebb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1057 > 1024). Running this sequence through the model will result in indexing errors\n",
      "C:\\Users\\CBS\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\CBS\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Extracting ALL-layer token embeddings (sliding window): 100%|██████████| 5/5 [00:23<00:00,  4.67s/it]\n",
      "Fitting PCA per layer: 100%|██████████| 49/49 [00:02<00:00, 24.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP2 DONE]\n",
      "Words: 957 | Tokens: 1057 | Layers: 49 | Hidden size: 1600\n",
      "Saved raw all-layer word embeddings: pieman_word_emb_all_layers_raw.npy\n",
      "Saved PCA(50) word embeddings:        pieman_word_emb_all_layers_pca50.npy\n",
      "Saved PCA models:                    pieman_pca_models.npz\n",
      "Saved PCA summary:                   pieman_pca_summary_by_layer.csv\n",
      "Saved figures: fig_pca_explained_variance_layerwise.png, fig_word_emb_norm_layerwise.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Step 2: Extract GPT2-XL embeddings for ALL layers and apply PCA(50) per layer.\n",
    "- Loads Step 1 word table to reuse word spans/timing and token-word mapping\n",
    "- Re-tokenizes transcript deterministically\n",
    "- Extracts token embeddings for all layers using sliding window\n",
    "- Pools token embeddings into word embeddings (mean pooling)\n",
    "- Fits PCA to 50 dims PER LAYER (recommended for layer-wise analyses)\n",
    "- Saves outputs (NPY/NPZ/CSV) and plots with unified ranges\n",
    "\n",
    "All comments are in English as requested.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# User config (EDIT THESE)\n",
    "# -----------------------------\n",
    "STEP1_DIR = r\"E:\\Nastase\\encoding_features\\pieman_step1\"     # contains pieman_word_table.csv from Step 1\n",
    "OUT_DIR   = r\"E:\\Nastase\\encoding_features\\pieman_step2\"     # output dir for Step 2\n",
    "MODEL_NAME = \"gpt2-xl\"\n",
    "\n",
    "MAX_CTX_TOKENS = 1024\n",
    "STRIDE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "PCA_DIM = 50\n",
    "\n",
    "# Plot settings (unified ranges)\n",
    "FIG_DPI = 220\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers (copied/compatible with Step 1)\n",
    "# -----------------------------\n",
    "def build_transcript_text(words: List[str]) -> Tuple[str, List[Tuple[int, int]]]:\n",
    "    spans = []\n",
    "    parts = []\n",
    "    cursor = 0\n",
    "    for i, w in enumerate(words):\n",
    "        if i > 0:\n",
    "            parts.append(\" \")\n",
    "            cursor += 1\n",
    "        start = cursor\n",
    "        parts.append(w)\n",
    "        cursor += len(w)\n",
    "        end = cursor\n",
    "        spans.append((start, end))\n",
    "    return \"\".join(parts), spans\n",
    "\n",
    "\n",
    "def map_tokens_to_words(offsets: List[Tuple[int, int]], word_spans: List[Tuple[int, int]]) -> List[List[int]]:\n",
    "    word_to_tokens: List[List[int]] = [[] for _ in word_spans]\n",
    "    w = 0\n",
    "    for t_idx, (t0, t1) in enumerate(offsets):\n",
    "        if t1 <= t0:\n",
    "            continue\n",
    "        while w < len(word_spans) and word_spans[w][1] <= t0:\n",
    "            w += 1\n",
    "        if w >= len(word_spans):\n",
    "            break\n",
    "        ww = w\n",
    "        while ww < len(word_spans):\n",
    "            w0, w1 = word_spans[ww]\n",
    "            if w0 >= t1:\n",
    "                break\n",
    "            if (t0 < w1) and (t1 > w0):\n",
    "                word_to_tokens[ww].append(t_idx)\n",
    "            ww += 1\n",
    "    return word_to_tokens\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_token_hiddenstates_all_layers_sliding_window(\n",
    "    model: AutoModel,\n",
    "    input_ids: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    max_ctx: int,\n",
    "    stride: int,\n",
    "    device: str,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract token embeddings for all layers using a sliding window.\n",
    "    Returns:\n",
    "      token_h: (L, T, H) float32\n",
    "        L = number of hidden-state layers returned by the model (embedding + each block)\n",
    "        T = number of tokens\n",
    "        H = hidden size\n",
    "\n",
    "    Note:\n",
    "      This stores all layers in RAM; for Pie Man (T~1057, H=1600, L=49),\n",
    "      it's manageable (~330MB float32). If you later do longer stories, we will stream per layer.\n",
    "    \"\"\"\n",
    "    T = int(input_ids.shape[0])\n",
    "    H = int(model.config.hidden_size)\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    # Determine number of layers by running a tiny forward once\n",
    "    test_out = model(\n",
    "        input_ids=input_ids[: min(8, T)].unsqueeze(0),\n",
    "        attention_mask=attention_mask[: min(8, T)].unsqueeze(0),\n",
    "        output_hidden_states=True,\n",
    "    )\n",
    "    L = len(test_out.hidden_states)  # includes embedding output\n",
    "    del test_out\n",
    "\n",
    "    token_h = np.zeros((L, T, H), dtype=np.float32)\n",
    "\n",
    "    for i in tqdm(range(0, T, stride), desc=\"Extracting ALL-layer token embeddings (sliding window)\"):\n",
    "        tgt_start = i\n",
    "        tgt_end = min(i + stride, T)\n",
    "\n",
    "        win_end = tgt_end\n",
    "        win_start = max(0, win_end - max_ctx)\n",
    "\n",
    "        ids_win = input_ids[win_start:win_end].unsqueeze(0)\n",
    "        mask_win = attention_mask[win_start:win_end].unsqueeze(0)\n",
    "\n",
    "        out = model(input_ids=ids_win, attention_mask=mask_win, output_hidden_states=True)\n",
    "        hs = out.hidden_states  # tuple length L, each (1, win_len, H)\n",
    "\n",
    "        rel_start = tgt_start - win_start\n",
    "        rel_end = tgt_end - win_start\n",
    "\n",
    "        for l in range(L):\n",
    "            token_h[l, tgt_start:tgt_end, :] = hs[l][0, rel_start:rel_end, :].detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    return token_h\n",
    "\n",
    "\n",
    "def plot_layerwise_curves(explained_cum: np.ndarray, out_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot cumulative explained variance of first PCA_DIM components per layer.\n",
    "    y-axis is fixed to [0, 1] for comparability.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(np.arange(len(explained_cum)), explained_cum)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.xlabel(\"Layer index (0=embeddings, 48=top)\")\n",
    "    plt.ylabel(f\"Cumulative explained variance (first {PCA_DIM} PCs)\")\n",
    "    plt.title(\"PCA(50) cumulative explained variance by layer\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"fig_pca_explained_variance_layerwise.png\"), dpi=FIG_DPI)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) Load Step 1 word table\n",
    "    # -----------------------------\n",
    "    step1_csv = os.path.join(STEP1_DIR, \"pieman_word_table.csv\")\n",
    "    word_df = pd.read_csv(step1_csv)\n",
    "\n",
    "    words = word_df[\"transcript_word\"].astype(str).tolist()\n",
    "    transcript_text, word_spans = build_transcript_text(words)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) Tokenize (offset mapping) and map tokens -> words\n",
    "    # -----------------------------\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "    enc = tokenizer(\n",
    "        transcript_text,\n",
    "        add_special_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"][0]\n",
    "    attention_mask = enc[\"attention_mask\"][0]\n",
    "    offsets = enc[\"offset_mapping\"][0].tolist()\n",
    "\n",
    "    word_to_tokens = map_tokens_to_words(offsets, word_spans)\n",
    "    n_tokens = int(input_ids.shape[0])\n",
    "\n",
    "    # Sanity: ensure no word is unmapped\n",
    "    n_unmapped = sum(1 for x in word_to_tokens if len(x) == 0)\n",
    "    if n_unmapped != 0:\n",
    "        raise RuntimeError(f\"{n_unmapped} words have no tokens mapped. This should be 0 for Pie Man.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) Load model and extract ALL-layer token hidden states\n",
    "    # -----------------------------\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    token_h = extract_token_hiddenstates_all_layers_sliding_window(\n",
    "        model=model,\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_ctx=MAX_CTX_TOKENS,\n",
    "        stride=STRIDE,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    L, T, H = token_h.shape\n",
    "    assert T == n_tokens\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) Token -> word mean pooling for ALL layers\n",
    "    # -----------------------------\n",
    "    W = len(word_to_tokens)\n",
    "    word_h = np.zeros((L, W, H), dtype=np.float32)\n",
    "\n",
    "    for w_idx, toks in enumerate(word_to_tokens):\n",
    "        toks_arr = np.array(toks, dtype=int)\n",
    "        # Mean over tokens for each layer\n",
    "        word_h[:, w_idx, :] = token_h[:, toks_arr, :].mean(axis=1).astype(np.float32)\n",
    "\n",
    "    # Save raw all-layer word embeddings (optional but useful)\n",
    "    np.save(os.path.join(OUT_DIR, \"pieman_word_emb_all_layers_raw.npy\"), word_h)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) PCA(50) per layer\n",
    "    # -----------------------------\n",
    "    word_pca = np.zeros((W, L, PCA_DIM), dtype=np.float32)\n",
    "    explained_cum = np.zeros((L,), dtype=np.float32)\n",
    "\n",
    "    # Store PCA params per layer for reproducibility\n",
    "    pca_means = np.zeros((L, H), dtype=np.float32)\n",
    "    pca_components = np.zeros((L, PCA_DIM, H), dtype=np.float32)\n",
    "    pca_explained = np.zeros((L, PCA_DIM), dtype=np.float32)\n",
    "\n",
    "    for l in tqdm(range(L), desc=\"Fitting PCA per layer\"):\n",
    "        X = word_h[l, :, :]  # (W, H)\n",
    "        pca = PCA(n_components=PCA_DIM, svd_solver=\"auto\", random_state=0)\n",
    "        Z = pca.fit_transform(X)  # (W, 50)\n",
    "\n",
    "        word_pca[:, l, :] = Z.astype(np.float32)\n",
    "        pca_means[l, :] = pca.mean_.astype(np.float32)\n",
    "        pca_components[l, :, :] = pca.components_.astype(np.float32)\n",
    "        pca_explained[l, :] = pca.explained_variance_ratio_.astype(np.float32)\n",
    "        explained_cum[l] = float(pca.explained_variance_ratio_[:PCA_DIM].sum())\n",
    "\n",
    "    np.save(os.path.join(OUT_DIR, \"pieman_word_emb_all_layers_pca50.npy\"), word_pca)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        os.path.join(OUT_DIR, \"pieman_pca_models.npz\"),\n",
    "        mean=pca_means,\n",
    "        components=pca_components,\n",
    "        explained_ratio=pca_explained,\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6) Save a compact CSV summary\n",
    "    # -----------------------------\n",
    "    summary = pd.DataFrame({\n",
    "        \"layer\": np.arange(L, dtype=int),\n",
    "        \"cum_explained_first50\": explained_cum,\n",
    "    })\n",
    "    summary.to_csv(os.path.join(OUT_DIR, \"pieman_pca_summary_by_layer.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7) Plot unified-range figures\n",
    "    # -----------------------------\n",
    "    plot_layerwise_curves(explained_cum, OUT_DIR)\n",
    "\n",
    "    # Also plot layerwise word embedding norms (raw) with unified y-range\n",
    "    norms = np.linalg.norm(word_h, axis=2)  # (L, W)\n",
    "    # Use a robust max for consistent plotting range\n",
    "    y_max = float(np.quantile(norms, 0.99))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.boxplot([norms[l, :] for l in range(L)], showfliers=False)\n",
    "    plt.ylim(0.0, y_max)\n",
    "    plt.xlabel(\"Layer index (1..L)\")\n",
    "    plt.ylabel(\"L2 norm (raw word embeddings)\")\n",
    "    plt.title(\"Distribution of word embedding norms by layer (raw)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"fig_word_emb_norm_layerwise.png\"), dpi=FIG_DPI)\n",
    "    plt.close()\n",
    "\n",
    "    print(\"\\n[STEP2 DONE]\")\n",
    "    print(f\"Words: {W} | Tokens: {T} | Layers: {L} | Hidden size: {H}\")\n",
    "    print(f\"Saved raw all-layer word embeddings: pieman_word_emb_all_layers_raw.npy\")\n",
    "    print(f\"Saved PCA(50) word embeddings:        pieman_word_emb_all_layers_pca50.npy\")\n",
    "    print(f\"Saved PCA models:                    pieman_pca_models.npz\")\n",
    "    print(f\"Saved PCA summary:                   pieman_pca_summary_by_layer.csv\")\n",
    "    print(\"Saved figures: fig_pca_explained_variance_layerwise.png, fig_word_emb_norm_layerwise.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d2e72",
   "metadata": {},
   "source": [
    "check sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6989b856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHAPES]\n",
      "raw: (49, 957, 1600)  (L,W,H)\n",
      "pca: (957, 49, 50)  (W,L,50)\n",
      "summary rows: 49\n",
      "\n",
      "[EXPLAINED VAR CHECK]\n",
      "min: 0.534538\n",
      "max: 0.86465245\n",
      "mean: 0.7680522302040815\n",
      "\n",
      "[NORM CHECK]\n",
      "norm p01: 1.2927607583999634\n",
      "norm p50: 140.78179931640625\n",
      "norm p99: 1063.14240234375\n",
      "\n",
      "[RED FLAGS]\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "STEP2_DIR = r\"E:\\Nastase\\encoding_features\\pieman_step2\"\n",
    "\n",
    "summary_csv = os.path.join(STEP2_DIR, \"pieman_pca_summary_by_layer.csv\")\n",
    "raw_path = os.path.join(STEP2_DIR, \"pieman_word_emb_all_layers_raw.npy\")\n",
    "pca_path = os.path.join(STEP2_DIR, \"pieman_word_emb_all_layers_pca50.npy\")\n",
    "\n",
    "summary = pd.read_csv(summary_csv)\n",
    "raw = np.load(raw_path)   # (L, W, H)\n",
    "pca = np.load(pca_path)   # (W, L, 50) in our code\n",
    "\n",
    "L, W, H = raw.shape\n",
    "Wp, Lp, D = pca.shape\n",
    "\n",
    "print(\"[SHAPES]\")\n",
    "print(\"raw:\", raw.shape, \" (L,W,H)\")\n",
    "print(\"pca:\", pca.shape, \" (W,L,50)\")\n",
    "print(\"summary rows:\", len(summary))\n",
    "\n",
    "print(\"\\n[EXPLAINED VAR CHECK]\")\n",
    "print(\"min:\", summary[\"cum_explained_first50\"].min())\n",
    "print(\"max:\", summary[\"cum_explained_first50\"].max())\n",
    "print(\"mean:\", summary[\"cum_explained_first50\"].mean())\n",
    "\n",
    "print(\"\\n[NORM CHECK]\")\n",
    "norms = np.linalg.norm(raw, axis=2)  # (L,W)\n",
    "print(\"norm p01:\", np.quantile(norms, 0.01))\n",
    "print(\"norm p50:\", np.quantile(norms, 0.50))\n",
    "print(\"norm p99:\", np.quantile(norms, 0.99))\n",
    "\n",
    "# Layerwise red-flag scan\n",
    "bad_layers = []\n",
    "for l in range(L):\n",
    "    x = raw[l]\n",
    "    if not np.isfinite(x).all():\n",
    "        bad_layers.append((l, \"non-finite\"))\n",
    "        continue\n",
    "    layer_norm = np.linalg.norm(x, axis=1)\n",
    "    if np.quantile(layer_norm, 0.99) > 10 * np.median(layer_norm):\n",
    "        bad_layers.append((l, \"heavy-tail\"))\n",
    "    if np.median(layer_norm) < 1e-6:\n",
    "        bad_layers.append((l, \"near-zero\"))\n",
    "\n",
    "print(\"\\n[RED FLAGS]\")\n",
    "print(\"none\" if len(bad_layers)==0 else bad_layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
